# Item Pipeline
2017-06-15 21:58:04 - 2017-06-15 22:38:03

使用的场景：
清除HTML数据；检验抓取的数据的有效性（确保items包含某些列）；
检验重复的数据（去重）；把数据存储到数据库

## Writing your own item pipeline
每个都是一个Python类，必须实现下列的方法：

`process_item(self, item, spider)`被每个管道构件调用，要么返回一个数据字典、
返回一个Item对象（或子类对象）、返回一个Twisted Deferred或触发DropItem异常。
参数：item，Item对象或者是字典--抓取的数据；spider抓取数据的爬虫对象。

`open_spider(self, spider)`启动爬虫的时候被调用，接受启用的爬虫对象为参数。

`close_spider(self, spider)`爬虫关闭的时候被调用

`from_crawler(cls, crawler)`？待了解？？？？

## 使用例子

检验价格是否有效，丢弃无效的价格数据
from scrapy.exceptions import DropItem

class PricePipeline(object):
    vat_factor = 1.15
    
    def process_item(self, item. spider):
    	if item['price']:
	    if item['price_exclude_vat']:
	        item['price'] = item['price'] * self.vat_factor
	    return item
	else:
	    raise DropItem("Missing price in %s" % item)
    
## Write items to a JSON file

import json

class JsonWriterPipeline(object):
    def open_spider(self, spider):
        self.file = open('items.jl', 'w')
    def close_spider(self, spider):
    	self.file.close()
    def process_item(self, item. spider):
    	line = json.dumps(dict(item)) + '\n'
	self.file.write(line)
	return item

## Write items to MongoDB
??

## 对Item截屏

import scrapy
import hashlib
from urllib.parse import quote

class ScreenshotPipeline(object):
    SPLASH_URL = 'http://localhost:8050/render.png?url={}'
    
    def process_item(self, item, spider):
        encoded_item_url = quote(item["url"])
        screenshot_url = self.SPLASH_URL.format(encoded_item_url)
        request = scrapy.Request(screenshot_url)
        dfd = spider.crawler.engine.download(request, spider)
        dfd.addBoth(self.return_item, item)
        return dfd

    def return_item(self, response, item):
        if response.status != 200:
            # Error happened, return item.
            return item

        # Save screenshot to file, filename will be hash of url.
        url = item["url"]
        url_hash = hashlib.md5(url.encode("utf8")).hexdigest()
        filename = "{}.png".format(url_hash)
        with open(filename, "wb") as f:
            f.write(response.body)

        # Store filename in item.
        item["screenshot_filename"] = filename
        return item

## 去重

from scrapy.exception import DropItem
class DuplicatesPipeline(object):
    def __init__(self):
        self.ids_seen = set()
    def process_item(self, item, spider):
        if item['id'] in self.ids_seen:
	    raise DropItem('Duplicate item found: %s' % item)
	else:
	    self.ids_seen.add(item['id'])
	    return item
	    
## Activating an Item Pipeline component
在配置文件中进行设置，值可以是0-1000间的数字：

ITEM_PIPELINE = {
    'myproject.pipelines.PricePipeline': 300, 
    'myproject.pipelines.JsonWriterPipeline': 800, 
}
