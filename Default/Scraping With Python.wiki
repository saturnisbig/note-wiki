# Scraping with Python

2016-11-16 20:16:22 - 2016-11-16 21:05:41

p1 - p14(13)

`pip install builtwith`, use builtwith to check the technologies a website used.

`pip install python-whois`, use `whois` to find out who owns a websites.

2016-11-16 21:36:26 - 2016-11-16 22:42:27

p14-p16(2)

2016-11-18 07:15:36 - 2016-11-18 07:32:38 

p17-p24(7)

2016-11-18 12:43:58 - 2016-11-18 13:08:04

p25-p34(9)

re

BeautifulSoup

lxml - css selector

2016-11-21 18:37:40 - 2016-11-21 19:51:55

install `cssselect`

2016-11-21 20:23:21 - 2016-11-21 22:17:30

### Chapter 3 - Caching Downloads

2016-11-24 12:48:35 - 2016-11-24 13:18:52 
p35-p47

### Chapter 2 - 
2016-12-12 06:43:43 - 2016-12-12 07:20:24
2016-12-13 21:24:57 - 
p25 - p34
2016-12-12 20:33:02 - 2016-12-12 21:26:29

2016-12-12 21:48:04 - 2016-12-12 22:06:08
复习第一章，重点是如何抓取网站所有链接地址

返回的状态码中：5XX是服务器问题，可以测试的地址'http://httpstat.us/500'

### Chapter 1 - 复习
2016-12-13 06:40:11 - 2016-12-13 07:19:39
p8-p16(9)

retry download
如果是服务器错误的问题，则可以再次尝试下载页面。'http://httpstat.us/500'来测试。

setting a user agent
在headers中设置，首先构建`request=urllib2.Request(url, headers)`，再通过urllib2来打开request。

sitemap crawler

ID iteration crawler
根据数据库自增ID爬取内容。
可能会碰到有些内容删除，ID不连续的情况；很多网站限制了通过ID访问；ID非数字的情况。
所以ID遍历爬虫也不实用

*Link crawler*

relative link问题；交叉链接导致在两个页面之间不停重复下载问题。

*Advanced features*

- parsing robots.txt

- support proxies

2016-12-13 21:25:17 - 2016-12-13 22:12:41
练习到链接爬虫

2016-12-14 20:47:05 - 2016-12-14 22:32:57
接着练习第一章

2016-12-19 21:16:05 - 2016-12-19 22:05:34
添加延迟下载，避免下载过快被封IP

2016-12-22 06:40:01 - 2016-12-22 07:27:03
2016-12-22 21:16:26 - 2016-12-22 23:12:00

## Chapter 3 缓存

2016-12-23 07:06:25 - 2016-12-23 07:31:58

- 添加缓存知识点
2016-12-26 20:56:16 - 2016-12-26 22:04:42

2016-12-26 22:45:47 - 2016-12-26 23:00:47


重新封装下载器为一个类，解决参数过多，同时加入支持缓存功能

`__call__`方法来实现

- [[磁盘缓存方法]]

练习 -- 封装downloader类 -- 复习

2016-12-27 06:29:19 - 2016-12-27 07:19:21
2016-12-27 20:38:02 - 2016-12-27 21:24:07

重写link_crawler
2016-12-27 21:24:20 - 2016-12-27 22:20

2016-12-29 21:44:35 - 2016-12-29 22:44:38
 

- 复习

2017-01-10 07:02:38 - 2017-01-10 07:33:06
